{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1 .What is the function of a summation junction of a neuron? What is threshold activation function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "summation junction is weighted sum of all the input nodes nΣi=1 (Wi*Xi)\n",
    "\n",
    "Threshold activation function\n",
    "it gives one/True as output if the input is postive or zero 0/False if the input is negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2. What is a step function? What is the difference of step function with threshold function?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "Threshold/step  activation function\n",
    "it gives one/True as output if the input is postive or zero 0/False if the input is negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3. Explain the McCulloch–Pitts model of neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "MP Neuron Model introduced by Warren McCulloch and Walter Pitts in 1943. MP neuron model is also known as linear threshold gate model.\n",
    "\n",
    "math of the same is as follows\n",
    "```\n",
    "g(x1,,x2,x3...xn) = nΣi=1 (Wi*Xi)\n",
    "\n",
    "y=f(g(x1,,x2,x3...xn)) =1 if g(x) >=θ\n",
    "\n",
    "                       =0 if g(x) < θ\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q4. Explain the ADALINE network model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it is an early single-layer artificial neural network.The network uses memistors It is based on the McCulloch–Pitts neuron. It consists of a weight, a bias and a summation function.\n",
    "\n",
    "The difference between Adaline and the standard (McCulloch–Pitts) perceptron is that in the learning phase, the weights are adjusted according to the weighted sum of the inputs (the net). In the standard perceptron, the net is passed to the activation (transfer) function and the function's output is used for adjusting the weights(origin of gradient descent).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q5 What is the constraint of a simple perceptron? Why it may fail with a real-world data set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "simple perceptron only works good when the data is linear separable in the real world data-set might not be linear. rather real-world data consist of complex pattern + outlier in such cases simple perceptron is bound to fail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q6 What is linearly inseparable problem? What is the role of the hidden layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "linear separability is a property of two sets of points. This is most easily visualized in two by thinking of one set of points as being colored blue and the other set of points as being colored red. These two sets are linearly separable if there exists at least one line in the plane with all of the blue points on one side of the line and all the red points on the other side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden layer increases the dimensions of learned feature vector.one neuron can separate point in one dimension another will use other dimension these output(feature vector) is mapped to output neuron layer which get features of input in multiple dimension space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q7 Explain XOR problem in case of a simple perceptron.\n",
    "# Answer\n",
    "The XOR problem is a common example of non-linearly separable problems simple perceptrons cannot solve as it can only works on data which linearly separable. multi-layer perceptrons are used and a common solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q8 Design a multi-layer perceptron to implement A XOR B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "Step1: Now for the corresponding weight vector $\\boldsymbol{w} : (\\boldsymbol{w_{1}}, \\boldsymbol{w_{2}})$ of the input vector $\\boldsymbol{x} : (\\boldsymbol{x_{1}}, \\boldsymbol{x_{2}})$ to the AND and OR node, the associated Perceptron Function can be defined as:\n",
    "  \\[$\\boldsymbol{\\hat{y}_{1}} = \\Theta\\left(w_{1} x_{1}+w_{2} x_{2}+b_{AND}\\right)$ \\]\n",
    "\n",
    "  \\[$\\boldsymbol{\\hat{y}_{2}} = \\Theta\\left(w_{1} x_{1}+w_{2} x_{2}+b_{OR}\\right)$ \\]\n",
    "\n",
    "Step2: The output ($\\boldsymbol{\\hat{y}}_{1}$) from the AND node will be inputed to the NOT node with weight $\\boldsymbol{w_{NOT}}$ and the associated Perceptron Function can be defined as:\n",
    "  \\[$\\boldsymbol{\\hat{y}_{3}} = \\Theta\\left(w_{NOT}  \\boldsymbol{\\hat{y}_{1}}+b_{NOT}\\right)$\\]\n",
    "\n",
    "Step3: The output ($\\boldsymbol{\\hat{y}}_{2}$) from the OR node and the output ($\\boldsymbol{\\hat{y}}_{3}$) from NOT node as mentioned in Step2 will be inputed to the AND node with weight $(\\boldsymbol{w_{AND1}}, \\boldsymbol{w_{AND2}})$. Then the corresponding output $\\boldsymbol{\\hat{y}}$ is the final output of the XOR logic function. The associated Perceptron Function can be defined as:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q9 Explain the single-layer feed forward architecture of ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "1. In this type of network, we have only two layers, i.e. input layer and output layer but the input layer does not count because no computation is performed in this layer.\n",
    "2. Output Layer is formed when different weights are applied on input nodes and the cumulative effect per node is taken.\n",
    "3. After this, the neurons collectively give the output layer to compute the output signals.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q10 Explain the competitive network architecture of ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Competitive learning is a form of unsupervised learning in artificial neural networks, in which nodes compete for the right to respond to a subset of the input data. A variant of Hebbian learning, competitive learning works by increasing the specialization of each node in the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q11 Consider a multi-layer feed forward neural network. Enumerate and explain steps in the backpropagation algorithm used to train the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "* Initialize all the weights with small random values;\n",
    "* Feed data into the network and figure out the value of the error function, obtained by comparison with the expected output value. \n",
    "* In order to minimize the error, the gradients of the error function with respect to each weight is calculated. now partial derivative is calculated for each weight using chain rule in derivatives we then perform weighted subtraction this weight is called learning rate.\n",
    "* Since the gradient vector has been calculated, each weight is updated in a iterative way, and we keep recalculating the gradients at the beginning of each training iteration step, until the error becomes lower than a certain established threshold, or the maximum number of iterations is reached, when finally the the algorithm ends and, hopefully, the network is well trained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q 12 What are the advantages and disadvantages of neural networks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answer\n",
    "Advantage:\n",
    "* given good amount of data and neural architecture ANN can give pretty good accuracy\n",
    "* It has fault tolerance:  Corruption of one or more cells of ANN does not prevent it from generating output. This feature makes the networks fault-tolerant. \n",
    "\n",
    "disadvantage :\n",
    "* requires huge amount of data to train on \n",
    "* required good amount of computation power to train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
